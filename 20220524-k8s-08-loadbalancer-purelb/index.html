

<!DOCTYPE html>
<html lang="en" data-default-color-scheme=auto>



<head>
  <meta charset="UTF-8">
  <link rel="apple-touch-icon" sizes="76x76" href="https://resource.tinychen.com/logos.png">
  <link rel="icon" href="https://resource.tinychen.com/logos.png">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=5.0, shrink-to-fit=no">
  <meta http-equiv="x-ua-compatible" content="ie=edge">
  
    <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests">
  
  <meta name="theme-color" content="#2f4154">
  <meta name="author" content="TinyChen">
  <meta name="keywords" content="">
  
    <meta name="description" content="本文主要在k8s原生集群上部署v0.6.1版本的PureLB作为k8s的LoadBalancer，主要涉及PureLB的Layer2模式和ECMP模式两种部署方案。由于PureLB的ECMP支持多种路由协议，这里选用的是在k8s中常见的BGP进行配置。由于BGP的相关原理和配置比较复杂，这里仅涉及简单的BGP配置。 文中使用的k8s集群是在CentOS7系统上基于docker和cilium组件部署">
<meta property="og:type" content="article">
<meta property="og:title" content="k8s系列08-负载均衡器之PureLB">
<meta property="og:url" content="https://tinychen.com/20220524-k8s-08-loadbalancer-purelb/index.html">
<meta property="og:site_name" content="TinyChen&#39;s Studio - 互联网技术学习工作经验分享">
<meta property="og:description" content="本文主要在k8s原生集群上部署v0.6.1版本的PureLB作为k8s的LoadBalancer，主要涉及PureLB的Layer2模式和ECMP模式两种部署方案。由于PureLB的ECMP支持多种路由协议，这里选用的是在k8s中常见的BGP进行配置。由于BGP的相关原理和配置比较复杂，这里仅涉及简单的BGP配置。 文中使用的k8s集群是在CentOS7系统上基于docker和cilium组件部署">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://resource.tinychen.com/202205241511819.jpg">
<meta property="article:published_time" content="2022-05-24T07:00:00.000Z">
<meta property="article:modified_time" content="2022-05-27T02:00:00.000Z">
<meta property="article:author" content="TinyChen">
<meta property="article:tag" content="k8s">
<meta property="article:tag" content="loadbalance">
<meta property="article:tag" content="metallb">
<meta property="article:tag" content="openelb">
<meta property="article:tag" content="purelb">
<meta property="article:tag" content="bgp">
<meta property="article:tag" content="quagga">
<meta property="article:tag" content="frr">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://resource.tinychen.com/202205241511819.jpg">
  
  
  <title>k8s系列08-负载均衡器之PureLB - TinyChen&#39;s Studio - 互联网技术学习工作经验分享</title>

  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/css/bootstrap.min.css" />


  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/github-markdown-css@4/github-markdown.min.css" />
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hint.css@2/hint.min.css" />

  
    
    
      
      <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10/styles/dracula.min.css" />
    
  

  
    <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.css" />
  


<!-- 主题依赖的图标库，不要自行修改 -->

<link rel="stylesheet" href="//at.alicdn.com/t/font_1749284_ba1fz6golrf.css">



<link rel="stylesheet" href="//at.alicdn.com/t/font_1736178_lbnruvf0jn.css">


<link  rel="stylesheet" href="/css/main.css" />

<!-- 自定义样式保持在最底部 -->

  
<link rel="stylesheet" href="/css/fluid-extention.css">



  <script id="fluid-configs">
    var Fluid = window.Fluid || {};
    var CONFIG = {"hostname":"tinychen.com","root":"/","version":"1.8.14","typing":{"enable":true,"typeSpeed":30,"cursorChar":"_","loop":false},"anchorjs":{"enable":true,"element":"h1,h2,h3,h4,h5,h6","placement":"right","visible":"hover","icon":""},"progressbar":{"enable":true,"height_px":3,"color":"#29d","options":{"showSpinner":false,"trickleSpeed":100}},"copy_btn":true,"image_zoom":{"enable":true,"img_url_replace":["",""]},"toc":{"enable":true,"headingSelector":"h1,h2,h3,h4,h5,h6","collapseDepth":0},"lazyload":{"enable":true,"loading_img":"/img/loading.gif","onlypost":false,"offset_factor":2},"web_analytics":{"enable":true,"baidu":"7a96963a1145ac7fde1442d739a11ffd","google":"UA-166769908-1","gtag":null,"tencent":{"sid":null,"cid":null},"woyaola":null,"cnzz":null,"leancloud":{"app_id":null,"app_key":null,"server_url":null,"path":"window.location.pathname","ignore_local":false}},"search_path":"/local-search.xml"};
  </script>
  <script  src="/js/utils.js" ></script>
  <script  src="/js/color-schema.js" ></script>
<meta name="generator" content="Hexo 5.4.0"><link rel="alternate" href="/atom.xml" title="TinyChen's Studio - 互联网技术学习工作经验分享" type="application/atom+xml">
</head>


<body>
  <header style="height: 70vh;">
    <nav id="navbar" class="navbar fixed-top  navbar-expand-lg navbar-dark scrolling-navbar">
  <div class="container">
    <a class="navbar-brand" href="/">
      <strong>TinyChen</strong>
    </a>

    <button id="navbar-toggler-btn" class="navbar-toggler" type="button" data-toggle="collapse"
            data-target="#navbarSupportedContent"
            aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
      <div class="animated-icon"><span></span><span></span><span></span></div>
    </button>

    <!-- Collapsible content -->
    <div class="collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav ml-auto text-center">
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/">
                <i class="iconfont icon-home-fill"></i>
                Home
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/archives/">
                <i class="iconfont icon-archive-fill"></i>
                Archives
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/categories/">
                <i class="iconfont icon-category-fill"></i>
                Categories
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/tags/">
                <i class="iconfont icon-tags-fill"></i>
                Tags
              </a>
            </li>
          
        
          
          
          
          
            <li class="nav-item">
              <a class="nav-link" href="/about/">
                <i class="iconfont icon-user-fill"></i>
                About
              </a>
            </li>
          
        
        
          <li class="nav-item" id="search-btn">
            <a class="nav-link" target="_self" href="javascript:;" data-toggle="modal" data-target="#modalSearch" aria-label="Search">
              &nbsp;<i class="iconfont icon-search"></i>&nbsp;
            </a>
          </li>
        
        
          <li class="nav-item" id="color-toggle-btn">
            <a class="nav-link" target="_self" href="javascript:;" aria-label="Color Toggle">&nbsp;<i
                class="iconfont icon-dark" id="color-toggle-icon"></i>&nbsp;</a>
          </li>
        
      </ul>
    </div>
  </div>
</nav>

    <div class="banner" id="banner" parallax=true
         style="background: url('https://resource.tinychen.com/202205241512640.jpg') no-repeat center center;
           background-size: cover;">
      <div class="full-bg-img">
        <div class="mask flex-center" style="background-color: rgba(0, 0, 0, 0.3)">
          <div class="page-header text-center fade-in-up">
            <span class="h2" id="subtitle" title="k8s系列08-负载均衡器之PureLB">
              
            </span>

            
              <div class="mt-3">
  
  
    <span class="post-meta">
      <i class="iconfont icon-date-fill" aria-hidden="true"></i>
      <time datetime="2022-05-24 15:00" pubdate>
        May 24, 2022 pm
      </time>
    </span>
  
</div>

<div class="mt-1">
  
    <span class="post-meta mr-2">
      <i class="iconfont icon-chart"></i>
      28k 字
    </span>
  

  

  
  
    
      <!-- 不蒜子统计文章PV -->
      <span id="busuanzi_container_page_pv" style="display: none">
        <i class="iconfont icon-eye" aria-hidden="true"></i>
        <span id="busuanzi_value_page_pv"></span> 次
      </span>
    
  
</div>

            
          </div>

          
        </div>
      </div>
    </div>
  </header>

  <main>
    
      

<div class="container-fluid nopadding-x">
  <div class="row nomargin-x">
    <div class="d-none d-lg-block col-lg-2"></div>
    <div class="col-lg-8 nopadding-x-md">
      <div class="container nopadding-x-md" id="board-ctn">
        <div class="py-5" id="board">
          <article class="post-content mx-auto">
            <!-- SEO header -->
            <h1 style="display: none">k8s系列08-负载均衡器之PureLB</h1>
            
              <p class="note note-info">
                
                  本文最后更新于：May 27, 2022 am
                
              </p>
            
            <div class="markdown-body">
              <p>本文主要在k8s原生集群上部署<code>v0.6.1</code>版本的<code>PureLB</code>作为k8s的<code>LoadBalancer</code>，主要涉及<strong>PureLB</strong>的<strong>Layer2模式</strong>和<strong>ECMP模式</strong>两种部署方案。由于PureLB的ECMP支持多种路由协议，这里选用的是在k8s中常见的BGP进行配置。由于BGP的相关原理和配置比较复杂，这里仅涉及简单的BGP配置。</p>
<p>文中使用的k8s集群是在CentOS7系统上基于<code>docker</code>和<code>cilium</code>组件部署<code>v1.23.6</code>版本，此前写的一些关于k8s基础知识和集群搭建的一些<a href="https://tinychen.com/tags/k8s/">方案</a>，有需要的同学可以看一下。</p>
<span id="more"></span>


<h1 id="1、工作原理"><a href="#1、工作原理" class="headerlink" title="1、工作原理"></a>1、工作原理</h1><p><a target="_blank" rel="noopener" href="https://purelb.gitlab.io/docs/how_it_works/howamiloadbalancing/">PureLB的工作原理</a>和其他的负载均衡器（MetalLB、OpenELB）类似，也可以大致分为Layer2模式和BGP模式，但是PureLB的两个模式和（MetalLB/OpenELB）还有着较大的区别。</p>
<blockquote>
<p>More simply, PureLB either uses the LoadBalancing functionality provided natively by k8s and/or combines k8s LoadBalancing with the routers Equal Cost Multipath (ECMP) load-balancing.</p>
</blockquote>
<ul>
<li>MetalLB/OpenELB的BGP模式是指通过跑BGP协议实现ECMP从而实现高可用，并且因为MetalLB/OpenELB只支持BGP这一个路由协议，所以称为BGP模式，或者也可以称之为ECMP模式；</li>
<li>PureLB会在k8s的宿主机节点上面添加一个新的虚拟网卡，通过这种方式使得我们可以使用Linux的网络栈看到k8s集群中使用的LoadBalancerVIP，同样得益于使用了Linux网络栈，因此PureLB可以使用任意路由协议实现ECMP（BGP、OSPF等），这种模式更倾向于ECMP模式而不止是BGP模式</li>
<li>MetalLB/OpenELB的Layer2模式会把所有的VIP的请求通过ARP/NDP吸引到一台节点上面，所有的流量都会经过这个节点，属于典型的<code>鸡蛋放在一个篮子里</code></li>
<li>PureLB的Layer2模式也和MetalLB/OpenELB不同，它可以根据单个VIP来选择节点，从而将多个VIP分散到集群中的不同节点上，<strong>尽可能</strong>的把流量均衡的分散到集群中的每个节点，一定程度上<code>将鸡蛋分散</code>，避免了严重的单点故障</li>
</ul>
<p>解释PureLB的工作原理比较简单，我们看一下官方的这个架构图：</p>
<p><img src="https://resource.tinychen.com/202205241423207.png" srcset="/img/loading.gif" lazyload></p>
<blockquote>
<p><em>Instead of thinking of PureLB as advertising services, think of PureLB as attracting packets to allocated addresses with KubeProxy forwarding those packets within the cluster via the Container Network Interface Network (POD Network) between nodes.</em></p>
</blockquote>
<ul>
<li><strong>Allocator</strong>：用来监听API中的<code>LoadBalancer</code>类型服务，并且负责分配IP。</li>
<li><strong>LBnodeagent</strong>： 作为<code>daemonset</code>部署到每个可以暴露请求并吸引流量的节点上，并且负责监听服务的状态变化同时负责把VIP添加到本地网卡或者是虚拟网卡</li>
<li><strong>KubeProxy</strong>：k8s的内置组件，并非是PureLB的一部分，但是PureLB依赖其进行正常工作，当对VIP的请求达到某个具体的节点之后，需要由kube-proxy来负责将其转发到对应的pod </li>
</ul>
<p>和MetalLB与OpenELB不同，PureLB并不需要自己去发送GARP/GNDP数据包，它执行的操作是<strong>把IP添加到k8s集群宿主机的网卡</strong>上面。具体来说就是：</p>
<ol>
<li>首先正常情况下每个机器上面都有一个本地网卡用于集群之间的常规通信，我们暂且称之为<code>eth0</code></li>
<li>然后PureLB会在每台机器上面创建一个虚拟网卡，默认名字为<code>kube-lb0</code></li>
<li>PureLB的<code>allocator</code>监听k8s-api中的<code>LoadBalancer</code>类型服务，并且负责分配IP</li>
<li>PureLB的<code>lbnodeagent</code>收到<code>allocator</code>分配的IP之后，开始对这个VIP进行判断</li>
<li>如果这个VIP和k8s宿主机是<strong>同网段的</strong>，那么会将其添加到本地网卡<code>eth0</code>上，此时我们可以在该节点上使用<code>ip addr show eth0</code>看到这个VIP</li>
<li>如果这个VIP和k8s宿主机是<strong>不同网段的</strong>，那么会将其添加到虚拟网卡<code>kube-lb0</code>上，此时我们可以在该节点上使用<code>ip addr show kube-lb0</code>看到这个VIP</li>
<li>一般来说Layer2模式的IP是和k8s宿主机节点同网段，ECMP模式是和k8s宿主机节点不同网段</li>
<li>接下来的发送<code>GARP/GNDP</code>数据包、路由协议通信等操作全部交给Linux网络栈自己或者是专门的路由软件（bird、frr等）实现，<strong>PureLB不需要参与这个过程</strong></li>
</ol>
<p>从上面这个逻辑我们不难看出：PureLB在设计实现原理的时候，尽可能地优先使用已有的基础架构设施。这样一来是可以尽可能地减少开发工作量，不必重复造轮子；二来是可以给用户提供尽可能多的接入选择，降低用户的入门门槛。</p>
<h1 id="2、Layer2模式"><a href="#2、Layer2模式" class="headerlink" title="2、Layer2模式"></a>2、Layer2模式</h1><h2 id="2-1-准备工作"><a href="#2-1-准备工作" class="headerlink" title="2.1 准备工作"></a>2.1 准备工作</h2><p>在开始部署PureLB之前，我们需要进行一些准备工作，主要就是端口检查和arp参数设置。</p>
<ul>
<li><p>PureLB使用了CRD，原生的k8s集群需要版本不小于1.15才能支持CRD</p>
</li>
<li><p>PureLB也使用了Memberlist来进行选主，因此需要确保<strong>7934端口</strong>没有被占用（<strong>包括TCP和UDP</strong>），否则会出现脑裂的情况</p>
<blockquote>
<p>PureLB uses a library called Memberlist to provide local network address failover faster than standard k8s timeouts would require. If you plan to use local network address and have applied firewalls to your nodes, it is necessary to add a rule to allow the memberlist election to occur. The port used by Memberlist in PureLB is <strong>Port 7934 UDP/TCP</strong>, memberlist uses both TCP and UDP, open both.</p>
</blockquote>
</li>
<li><p>修改arp参数，和其他的开源LoadBalancer一样，也要把kube-proxy的arp参数设置为严格<code>strictARP: true</code></p>
<blockquote>
<p>把k8s集群中的ipvs配置打开<a target="_blank" rel="noopener" href="https://kubernetes.io/docs/reference/config-api/kube-proxy-config.v1alpha1/"><code>strictARP</code></a>之后，k8s集群中的<code>kube-proxy</code>会停止响应<code>kube-ipvs0</code>网卡之外的其他网卡的arp请求。</p>
<p><code>strict ARP</code>开启之后相当于把 将 <code>arp_ignore</code> 设置为 1 并将 <code>arp_announce</code> 设置为 2 启用严格的 ARP，这个原理和LVS中的DR模式对RS的配置一样，可以参考之前的<a href="https://tinychen.com/20200427-lvs-principle-introduction/#6%E3%80%81ARP-in-LVS">文章中的解释</a>。</p>
</blockquote>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># 查看kube-proxy中的strictARP配置</span><br>$ kubectl get configmap -n kube-system kube-proxy -o yaml | grep strictARP<br>      strictARP: <span class="hljs-literal">false</span><br><br><span class="hljs-comment"># 手动修改strictARP配置为true</span><br>$ kubectl edit configmap -n kube-system kube-proxy<br>configmap/kube-proxy edited<br><br><span class="hljs-comment"># 使用命令直接修改并对比不同</span><br>$ kubectl get configmap kube-proxy -n kube-system -o yaml | sed -e <span class="hljs-string">&quot;s/strictARP: false/strictARP: true/&quot;</span> | kubectl diff -f - -n kube-system<br><br><span class="hljs-comment"># 确认无误后使用命令直接修改并生效</span><br>$ kubectl get configmap kube-proxy -n kube-system -o yaml | sed -e <span class="hljs-string">&quot;s/strictARP: false/strictARP: true/&quot;</span> | kubectl apply -f - -n kube-system<br><br><span class="hljs-comment"># 重启kube-proxy确保配置生效</span><br>$ kubectl rollout restart ds kube-proxy -n kube-system<br><br><span class="hljs-comment"># 确认配置生效</span><br>$ kubectl get configmap -n kube-system kube-proxy -o yaml | grep strictARP<br>      strictARP: <span class="hljs-literal">true</span><br></code></pre></div></td></tr></table></figure></li>
</ul>
<h2 id="2-2-部署PureLB"><a href="#2-2-部署PureLB" class="headerlink" title="2.2 部署PureLB"></a>2.2 部署PureLB</h2><p>老规矩我们还是使用manifest文件进行部署，当然官方还提供了<a target="_blank" rel="noopener" href="https://purelb.gitlab.io/docs/install/">helm等部署方式</a>。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ wget https://gitlab.com/api/v4/projects/purelb%2Fpurelb/packages/generic/manifest/0.0.1/purelb-complete.yaml<br><br>$ kubectl apply -f purelb/purelb-complete.yaml<br>namespace/purelb created<br>customresourcedefinition.apiextensions.k8s.io/lbnodeagents.purelb.io created<br>customresourcedefinition.apiextensions.k8s.io/servicegroups.purelb.io created<br>serviceaccount/allocator created<br>serviceaccount/lbnodeagent created<br>Warning: policy/v1beta1 PodSecurityPolicy is deprecated <span class="hljs-keyword">in</span> v1.21+, unavailable <span class="hljs-keyword">in</span> v1.25+<br>podsecuritypolicy.policy/allocator created<br>podsecuritypolicy.policy/lbnodeagent created<br>role.rbac.authorization.k8s.io/pod-lister created<br>clusterrole.rbac.authorization.k8s.io/purelb:allocator created<br>clusterrole.rbac.authorization.k8s.io/purelb:lbnodeagent created<br>rolebinding.rbac.authorization.k8s.io/pod-lister created<br>clusterrolebinding.rbac.authorization.k8s.io/purelb:allocator created<br>clusterrolebinding.rbac.authorization.k8s.io/purelb:lbnodeagent created<br>deployment.apps/allocator created<br>daemonset.apps/lbnodeagent created<br>error: unable to recognize <span class="hljs-string">&quot;purelb/purelb-complete.yaml&quot;</span>: no matches <span class="hljs-keyword">for</span> kind <span class="hljs-string">&quot;LBNodeAgent&quot;</span> <span class="hljs-keyword">in</span> version <span class="hljs-string">&quot;purelb.io/v1&quot;</span><br><br>$ kubectl apply -f purelb/purelb-complete.yaml<br>namespace/purelb unchanged<br>customresourcedefinition.apiextensions.k8s.io/lbnodeagents.purelb.io configured<br>customresourcedefinition.apiextensions.k8s.io/servicegroups.purelb.io configured<br>serviceaccount/allocator unchanged<br>serviceaccount/lbnodeagent unchanged<br>Warning: policy/v1beta1 PodSecurityPolicy is deprecated <span class="hljs-keyword">in</span> v1.21+, unavailable <span class="hljs-keyword">in</span> v1.25+<br>podsecuritypolicy.policy/allocator configured<br>podsecuritypolicy.policy/lbnodeagent configured<br>role.rbac.authorization.k8s.io/pod-lister unchanged<br>clusterrole.rbac.authorization.k8s.io/purelb:allocator unchanged<br>clusterrole.rbac.authorization.k8s.io/purelb:lbnodeagent unchanged<br>rolebinding.rbac.authorization.k8s.io/pod-lister unchanged<br>clusterrolebinding.rbac.authorization.k8s.io/purelb:allocator unchanged<br>clusterrolebinding.rbac.authorization.k8s.io/purelb:lbnodeagent unchanged<br>deployment.apps/allocator unchanged<br>daemonset.apps/lbnodeagent unchanged<br>lbnodeagent.purelb.io/default created<br><br></code></pre></div></td></tr></table></figure>


<blockquote>
<p>请注意，由于 Kubernetes 的最终一致性架构，此manifest清单的第一个应用程序可能会失败。发生这种情况是因为清单既定义了CRD，又使用该CRD创建了资源。如果发生这种情况，请再次应用manifest清单，应该就会部署成功。</p>
<p>Please note that due to Kubernetes’ eventually-consistent architecture the first application of this manifest can fail. This happens because the manifest both defines a Custom Resource Definition and creates a resource using that definition. If this happens then apply the manifest again and it should succeed because Kubernetes will have processed the definition in the mean time.</p>
</blockquote>
<p>检查一下部署的服务</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl get pods -n purelb -o wide<br>NAME                         READY   STATUS    RESTARTS   AGE   IP             NODE                                       NOMINATED NODE   READINESS GATES<br>allocator-5bf9ddbf9b-p976d   1/1     Running   0          2m    10.0.2.140     tiny-cilium-worker-188-12.k8s.tcinternal   &lt;none&gt;           &lt;none&gt;<br>lbnodeagent-df2hn            1/1     Running   0          2m    10.31.188.12   tiny-cilium-worker-188-12.k8s.tcinternal   &lt;none&gt;           &lt;none&gt;<br>lbnodeagent-jxn9h            1/1     Running   0          2m    10.31.188.1    tiny-cilium-master-188-1.k8s.tcinternal    &lt;none&gt;           &lt;none&gt;<br>lbnodeagent-xn8dz            1/1     Running   0          2m    10.31.188.11   tiny-cilium-worker-188-11.k8s.tcinternal   &lt;none&gt;           &lt;none&gt;<br><br>$ kubectl get deploy -n purelb<br>NAME        READY   UP-TO-DATE   AVAILABLE   AGE<br>allocator   1/1     1            1           10m<br>[root@tiny-cilium-master-188-1 purelb]<span class="hljs-comment"># kubectl get ds -n purelb</span><br>NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE<br>lbnodeagent   3         3         3       3            3           kubernetes.io/os=linux   10m<br><br>$ kubectl get crd | grep purelb<br>lbnodeagents.purelb.io                       2022-05-20T06:42:01Z<br>servicegroups.purelb.io                      2022-05-20T06:42:01Z<br><br>$ kubectl get --namespace=purelb servicegroups.purelb.io<br>No resources found <span class="hljs-keyword">in</span> purelb namespace.<br>$ kubectl get --namespace=purelb lbnodeagent.purelb.io<br>NAME      AGE<br>default   55m<br></code></pre></div></td></tr></table></figure>

<p>和MetalLB/OpenELB不一样的是，PureLB使用了另外的一个单独的虚拟网卡<code>kube-lb0</code>而不是默认的<code>kube-ipvs0</code>网卡</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ ip addr show kube-lb0<br>15: kube-lb0: &lt;BROADCAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000<br>    link/ether 12:27:b1:48:4e:3a brd ff:ff:ff:ff:ff:ff<br>    inet6 fe80::1027:b1ff:fe48:4e3a/64 scope link<br>       valid_lft forever preferred_lft forever<br></code></pre></div></td></tr></table></figure>

<h2 id="2-3-配置purelb"><a href="#2-3-配置purelb" class="headerlink" title="2.3 配置purelb"></a>2.3 配置purelb</h2><p>上面部署的时候我们知道purelb主要创建了两个CRD，分别是<code>lbnodeagents.purelb.io</code>和<code>servicegroups.purelb.io</code></p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl api-resources --api-group=purelb.io<br>NAME            SHORTNAMES   APIVERSION     NAMESPACED   KIND<br>lbnodeagents    lbna,lbnas   purelb.io/v1   <span class="hljs-literal">true</span>         LBNodeAgent<br>servicegroups   sg,sgs       purelb.io/v1   <span class="hljs-literal">true</span>         ServiceGroup<br></code></pre></div></td></tr></table></figure>



<h3 id="2-3-1-lbnodeagents-purelb-io"><a href="#2-3-1-lbnodeagents-purelb-io" class="headerlink" title="2.3.1 lbnodeagents.purelb.io"></a>2.3.1 lbnodeagents.purelb.io</h3><p>默认情况下已经创建好了一个名为<code>default</code>的<code>lbnodeagent</code>，我们可以看一下它的几个配置项</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl describe --namespace=purelb lbnodeagent.purelb.io/default<br>Name:         default<br>Namespace:    purelb<br>Labels:       &lt;none&gt;<br>Annotations:  &lt;none&gt;<br>API Version:  purelb.io/v1<br>Kind:         LBNodeAgent<br>Metadata:<br>  Creation Timestamp:  2022-05-20T06:42:23Z<br>  Generation:          1<br>  Managed Fields:<br>    API Version:  purelb.io/v1<br>    Fields Type:  FieldsV1<br>    fieldsV1:<br>      f:metadata:<br>        f:annotations:<br>          .:<br>          f:kubectl.kubernetes.io/last-applied-configuration:<br>      f:spec:<br>        .:<br>        f:<span class="hljs-built_in">local</span>:<br>          .:<br>          f:extlbint:<br>          f:localint:<br>    Manager:         kubectl-client-side-apply<br>    Operation:       Update<br>    Time:            2022-05-20T06:42:23Z<br>  Resource Version:  1765489<br>  UID:               59f0ad8c-1024-4432-8f95-9ad574b28fff<br>Spec:<br>  Local:<br>    Extlbint:  kube-lb0<br>    Localint:  default<br>Events:        &lt;none&gt;<br></code></pre></div></td></tr></table></figure>

<p>注意上面的<code>Spec:Local:</code>字段中的<code>Extlbint</code>和<code>Localint</code></p>
<ul>
<li><code>Extlbint</code>字段指定的是PureLB使用的虚拟网卡名称，默认为<code>kube-lb0</code>，如果修改为自定义名称，记得同时修改bird中的配置</li>
<li><code>Localint</code>字段指定的是用来实际通信的物理网卡，默认情况下会使用正则表达式来匹配，当然也可以自定义，如果集群节点是单网卡机器基本无需修改</li>
</ul>
<h3 id="2-3-2-servicegroups-purelb-io"><a href="#2-3-2-servicegroups-purelb-io" class="headerlink" title="2.3.2 servicegroups.purelb.io"></a>2.3.2 servicegroups.purelb.io</h3><p><code>servicegroups</code>默认情况下并没有创建，需要我们进行手动配置，注意purellb是支持ipv6的，配置方式和ipv4一致，只是这里没有需求就没有单独配置v6pool。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">apiVersion: purelb.io/v1<br>kind: ServiceGroup<br>metadata:<br>  name: layer2-ippool<br>  namespace: purelb<br>spec:<br>  <span class="hljs-built_in">local</span>:<br>    v4pool:<br>      subnet: <span class="hljs-string">&#x27;10.31.188.64/26&#x27;</span><br>      pool: <span class="hljs-string">&#x27;10.31.188.64-10.31.188.126&#x27;</span><br>      aggregation: /32<br></code></pre></div></td></tr></table></figure>

<p>然后我们直接部署并检查</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl apply -f purelb-ipam.yaml<br>servicegroup.purelb.io/layer2-ippool created<br><br>$ kubectl get sg -n purelb<br>NAME            AGE<br>layer2-ippool   50s<br><br>$ kubectl describe sg -n purelb<br>Name:         layer2-ippool<br>Namespace:    purelb<br>Labels:       &lt;none&gt;<br>Annotations:  &lt;none&gt;<br>API Version:  purelb.io/v1<br>Kind:         ServiceGroup<br>Metadata:<br>  Creation Timestamp:  2022-05-20T07:58:32Z<br>  Generation:          1<br>  Managed Fields:<br>    API Version:  purelb.io/v1<br>    Fields Type:  FieldsV1<br>    fieldsV1:<br>      f:metadata:<br>        f:annotations:<br>          .:<br>          f:kubectl.kubernetes.io/last-applied-configuration:<br>      f:spec:<br>        .:<br>        f:<span class="hljs-built_in">local</span>:<br>          .:<br>          f:v4pool:<br>            .:<br>            f:aggregation:<br>            f:pool:<br>            f:subnet:<br>    Manager:         kubectl-client-side-apply<br>    Operation:       Update<br>    Time:            2022-05-20T07:58:32Z<br>  Resource Version:  1774182<br>  UID:               92422ea9-231d-4280-a8b5-ec6c61605dd9<br>Spec:<br>  Local:<br>    v4pool:<br>      Aggregation:  /32<br>      Pool:         10.31.188.64-10.31.188.126<br>      Subnet:       10.31.188.64/26<br>Events:<br>  Type    Reason  Age    From              Message<br>  ----    ------  ----   ----              -------<br>  Normal  Parsed  4m13s  purelb-allocator  ServiceGroup parsed successfully<br></code></pre></div></td></tr></table></figure>

<h2 id="2-4-部署service"><a href="#2-4-部署service" class="headerlink" title="2.4 部署service"></a>2.4 部署service</h2><p>PureLB的部分CRD特性需要我们手动<a target="_blank" rel="noopener" href="https://purelb.gitlab.io/docs/operation/services/">在Service中通过添加注解（annotations）</a>来启用，这里我们只需要指定<code>purelb.io/service-group</code>来确定使用的IP池即可</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">annotations:</span><br>  <span class="hljs-attr">purelb.io/service-group:</span> <span class="hljs-string">layer2-ippool</span><br></code></pre></div></td></tr></table></figure>

<p>完整的测试服务相关manifest如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-quic</span><br><br><span class="hljs-meta">---</span><br><span class="hljs-meta"></span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-lb</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">matchLabels:</span><br>      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-lb</span><br>  <span class="hljs-attr">replicas:</span> <span class="hljs-number">4</span><br>  <span class="hljs-attr">template:</span><br>    <span class="hljs-attr">metadata:</span><br>      <span class="hljs-attr">labels:</span><br>        <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-lb</span><br>    <span class="hljs-attr">spec:</span><br>      <span class="hljs-attr">containers:</span><br>      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-lb</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">tinychen777/nginx-quic:latest</span><br>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span><br>        <span class="hljs-attr">ports:</span><br>        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span><br><br><span class="hljs-meta">---</span><br><span class="hljs-meta"></span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">purelb.io/service-group:</span> <span class="hljs-string">layer2-ippool</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-lb-service</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">allocateLoadBalancerNodePorts:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">externalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">internalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-lb</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for service access port</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for pod access port</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">LoadBalancer</span><br><br><span class="hljs-meta">---</span><br><span class="hljs-meta"></span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">purelb.io/service-group:</span> <span class="hljs-string">layer2-ippool</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-lb2-service</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">allocateLoadBalancerNodePorts:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">externalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">internalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-lb</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for service access port</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for pod access port</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">LoadBalancer</span><br><br>  <br><span class="hljs-meta">---</span><br><span class="hljs-meta"></span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">purelb.io/service-group:</span> <span class="hljs-string">layer2-ippool</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-lb3-service</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">allocateLoadBalancerNodePorts:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">externalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">internalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-lb</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for service access port</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for pod access port</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">LoadBalancer</span><br></code></pre></div></td></tr></table></figure>

<p>确认没有问题之后我们直接部署，会创建<code>namespace/nginx-quic</code>、<code>deployment.apps/nginx-lb</code>、<code>service/nginx-lb-service</code> 、<code>service/nginx-lb2-service</code> 、<code>service/nginx-lb3-service</code> 这几个资源</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl apply -f nginx-quic-lb.yaml<br>namespace/nginx-quic unchanged<br>deployment.apps/nginx-lb created<br>service/nginx-lb-service created<br>service/nginx-lb2-service created<br>service/nginx-lb3-service created<br><br>$ kubectl get svc -n nginx-quic<br>NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)          AGE<br>nginx-lb-service     LoadBalancer   10.188.54.81    10.31.188.64   80/TCP           101s<br>nginx-lb2-service    LoadBalancer   10.188.34.171   10.31.188.65   80/TCP           101s<br>nginx-lb3-service    LoadBalancer   10.188.6.24     10.31.188.66   80/TCP           101s<br></code></pre></div></td></tr></table></figure>

<p>查看k8s的服务日志就能知道VIP在哪个节点上</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl describe service nginx-lb-service -n nginx-quic<br>Name:                     nginx-lb-service<br>Namespace:                nginx-quic<br>Labels:                   &lt;none&gt;<br>Annotations:              purelb.io/allocated-by: PureLB<br>                          purelb.io/allocated-from: layer2-ippool<br>                          purelb.io/announcing-IPv4: tiny-cilium-worker-188-11.k8s.tcinternal,eth0<br>                          purelb.io/service-group: layer2-ippool<br>Selector:                 app=nginx-lb<br>Type:                     LoadBalancer<br>IP Family Policy:         SingleStack<br>IP Families:              IPv4<br>IP:                       10.188.54.81<br>IPs:                      10.188.54.81<br>LoadBalancer Ingress:     10.31.188.64<br>Port:                     &lt;<span class="hljs-built_in">unset</span>&gt;  80/TCP<br>TargetPort:               80/TCP<br>Endpoints:                10.0.1.45:80,10.0.1.49:80,10.0.2.181:80 + 1 more...<br>Session Affinity:         None<br>External Traffic Policy:  Cluster<br>Events:<br>  Type    Reason           Age                   From                Message<br>  ----    ------           ----                  ----                -------<br>  Normal  AddressAssigned  3m12s                 purelb-allocator    Assigned &#123;Ingress:[&#123;IP:10.31.188.64 Hostname: Ports:[]&#125;]&#125; from pool layer2-ippool<br>  Normal  AnnouncingLocal  3m8s (x7 over 3m12s)  purelb-lbnodeagent  Node tiny-cilium-worker-188-11.k8s.tcinternal announcing 10.31.188.64 on interface eth0<br>  <br>$ kubectl describe service nginx-lb2-service -n nginx-quic<br>Name:                     nginx-lb2-service<br>Namespace:                nginx-quic<br>Labels:                   &lt;none&gt;<br>Annotations:              purelb.io/allocated-by: PureLB<br>                          purelb.io/allocated-from: layer2-ippool<br>                          purelb.io/announcing-IPv4: tiny-cilium-master-188-1.k8s.tcinternal,eth0<br>                          purelb.io/service-group: layer2-ippool<br>Selector:                 app=nginx-lb<br>Type:                     LoadBalancer<br>IP Family Policy:         SingleStack<br>IP Families:              IPv4<br>IP:                       10.188.34.171<br>IPs:                      10.188.34.171<br>LoadBalancer Ingress:     10.31.188.65<br>Port:                     &lt;<span class="hljs-built_in">unset</span>&gt;  80/TCP<br>TargetPort:               80/TCP<br>Endpoints:                10.0.1.45:80,10.0.1.49:80,10.0.2.181:80 + 1 more...<br>Session Affinity:         None<br>External Traffic Policy:  Cluster<br>Events:<br>  Type    Reason           Age                    From                Message<br>  ----    ------           ----                   ----                -------<br>  Normal  AddressAssigned  4m20s                  purelb-allocator    Assigned &#123;Ingress:[&#123;IP:10.31.188.65 Hostname: Ports:[]&#125;]&#125; from pool layer2-ippool<br>  Normal  AnnouncingLocal  4m17s (x5 over 4m20s)  purelb-lbnodeagent  Node tiny-cilium-master-188-1.k8s.tcinternal announcing 10.31.188.65 on interface eth0<br><br>$ kubectl describe service nginx-lb3-service -n nginx-quic<br>Name:                     nginx-lb3-service<br>Namespace:                nginx-quic<br>Labels:                   &lt;none&gt;<br>Annotations:              purelb.io/allocated-by: PureLB<br>                          purelb.io/allocated-from: layer2-ippool<br>                          purelb.io/announcing-IPv4: tiny-cilium-worker-188-11.k8s.tcinternal,eth0<br>                          purelb.io/service-group: layer2-ippool<br>Selector:                 app=nginx-lb<br>Type:                     LoadBalancer<br>IP Family Policy:         SingleStack<br>IP Families:              IPv4<br>IP:                       10.188.6.24<br>IPs:                      10.188.6.24<br>LoadBalancer Ingress:     10.31.188.66<br>Port:                     &lt;<span class="hljs-built_in">unset</span>&gt;  80/TCP<br>TargetPort:               80/TCP<br>Endpoints:                10.0.1.45:80,10.0.1.49:80,10.0.2.181:80 + 1 more...<br>Session Affinity:         None<br>External Traffic Policy:  Cluster<br>Events:<br>  Type    Reason           Age                    From                Message<br>  ----    ------           ----                   ----                -------<br>  Normal  AddressAssigned  4m33s                  purelb-allocator    Assigned &#123;Ingress:[&#123;IP:10.31.188.66 Hostname: Ports:[]&#125;]&#125; from pool layer2-ippool<br>  Normal  AnnouncingLocal  4m29s (x6 over 4m33s)  purelb-lbnodeagent  Node tiny-cilium-worker-188-11.k8s.tcinternal announcing 10.31.188.66 on interface eth0<br></code></pre></div></td></tr></table></figure>

<p>我们找一台局域网内的其他机器查看可以发现三个VIP的mac地址并不完全一样，符合上面的日志输出结果</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ ip neigh | grep 10.31.188.6<br>10.31.188.65 dev eth0 lladdr 52:54:00:69:0a:ab REACHABLE<br>10.31.188.64 dev eth0 lladdr 52:54:00:3c:88:cb REACHABLE<br>10.31.188.66 dev eth0 lladdr 52:54:00:3c:88:cb REACHABLE<br></code></pre></div></td></tr></table></figure>

<p>我们再查看节点上面的网络地址，除了大家都有的kube-ipvs0网卡上面有所有的VIP，PureLB和MetalLB/OpenELB最大的不同在于PureLB还能在对应节点的物理网卡上面准确地看到对应的Service所属的VIP。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ ansible cilium -m <span class="hljs-built_in">command</span> -a <span class="hljs-string">&quot;ip addr show eth0&quot;</span><br>10.31.188.11 | CHANGED | rc=0 &gt;&gt;<br>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000<br>    link/ether 52:54:00:3c:88:cb brd ff:ff:ff:ff:ff:ff<br>    inet 10.31.188.11/16 brd 10.31.255.255 scope global noprefixroute eth0<br>       valid_lft forever preferred_lft forever<br>    inet 10.31.188.64/16 brd 10.31.255.255 scope global secondary eth0<br>       valid_lft forever preferred_lft forever<br>    inet 10.31.188.66/16 brd 10.31.255.255 scope global secondary eth0<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::5054:ff:fe3c:88cb/64 scope link<br>       valid_lft forever preferred_lft forever<br><br>10.31.188.12 | CHANGED | rc=0 &gt;&gt;<br>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000<br>    link/ether 52:54:00:32:a7:42 brd ff:ff:ff:ff:ff:ff<br>    inet 10.31.188.12/16 brd 10.31.255.255 scope global noprefixroute eth0<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::5054:ff:fe32:a742/64 scope link<br>       valid_lft forever preferred_lft forever<br><br>10.31.188.1 | CHANGED | rc=0 &gt;&gt;<br>2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000<br>    link/ether 52:54:00:69:0a:ab brd ff:ff:ff:ff:ff:ff<br>    inet 10.31.188.1/16 brd 10.31.255.255 scope global noprefixroute eth0<br>       valid_lft forever preferred_lft forever<br>    inet 10.31.188.65/16 brd 10.31.255.255 scope global secondary eth0<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::5054:ff:fe69:aab/64 scope link<br>       valid_lft forever preferred_lft forever<br></code></pre></div></td></tr></table></figure>

<h2 id="2-5-指定VIP"><a href="#2-5-指定VIP" class="headerlink" title="2.5 指定VIP"></a>2.5 指定VIP</h2><p>同样的，需要指定IP的话我们可以添加<code>spec:loadBalancerIP:</code>字段来指定VIP</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">purelb.io/service-group:</span> <span class="hljs-string">layer2-ippool</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-lb4-service</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">allocateLoadBalancerNodePorts:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">externalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">internalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-lb</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for service access port</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for pod access port</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">LoadBalancer</span><br>  <span class="hljs-attr">loadBalancerIP:</span> <span class="hljs-number">10.31</span><span class="hljs-number">.188</span><span class="hljs-number">.100</span><br></code></pre></div></td></tr></table></figure>

<h2 id="2-6-关于nodeport"><a href="#2-6-关于nodeport" class="headerlink" title="2.6 关于nodeport"></a>2.6 关于nodeport</h2><p>PureLB支持<code>allocateLoadBalancerNodePorts</code>特性，可以通过设置<code>allocateLoadBalancerNodePorts: false</code>来<strong>关闭自动为LoadBalancer服务分配nodeport</strong>这个功能。</p>
<h1 id="3、ECMP模式"><a href="#3、ECMP模式" class="headerlink" title="3、ECMP模式"></a>3、ECMP模式</h1><p>因为purelb使用了Linux的网络栈，因此在ECMP的实现这一块就有更多的选择，这里我们参考官方的实现方案，使用<a target="_blank" rel="noopener" href="https://gitlab.com/purelb/bird_router">BGP+Bird</a>的方案来实现。</p>
<table>
<thead>
<tr>
<th align="center">IP</th>
<th align="center">Hostname</th>
</tr>
</thead>
<tbody><tr>
<td align="center">10.31.188.1</td>
<td align="center">tiny-cilium-master-188-1.k8s.tcinternal</td>
</tr>
<tr>
<td align="center">10.31.188.11</td>
<td align="center">tiny-cilium-worker-188-11.k8s.tcinternal</td>
</tr>
<tr>
<td align="center">10.31.188.12</td>
<td align="center">tiny-cilium-worker-188-12.k8s.tcinternal</td>
</tr>
<tr>
<td align="center">10.188.0.0/18</td>
<td align="center">serviceSubnet</td>
</tr>
<tr>
<td align="center">10.31.254.251</td>
<td align="center">BGP-Router(frr)</td>
</tr>
<tr>
<td align="center">10.189.0.0/16</td>
<td align="center">PuerLB-BGP-IPpool</td>
</tr>
</tbody></table>
<p>其中PureLB的ASN是64515，路由器的ASN为64512。</p>
<h2 id="3-1-准备工作"><a href="#3-1-准备工作" class="headerlink" title="3.1 准备工作"></a>3.1 准备工作</h2><p>我们先把官方的GitHub仓库拉到本地，然后实际上我们部署需要的配置文件只有<code>bird-cm.yml</code>和<code>bird.yml</code>这两个即可。</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ git <span class="hljs-built_in">clone</span> https://gitlab.com/purelb/bird_router.git<br>$ ls bird*yml<br>bird-cm.yml  bird.yml<br></code></pre></div></td></tr></table></figure>

<p>接下来我们对其进行一些修改，首先是configmap文件<code>bird-cm.yml</code>，我们只需要修改<code>description</code>、<code>as</code>、<code>neighbor</code>这三个字段：</p>
<ul>
<li><code>description</code>：建立BGP连接的路由器的描述，一般我习惯命名为IP的数字加横杠</li>
<li><code>as</code>：自己的ASN</li>
<li><code>neighbor</code>：建立BGP连接的路由器的IP地址</li>
<li><code>namespace</code>：官方默认新建了一个<code>router</code>的<code>namespace</code>来管理，这里我们为了方便统一到<code>purelb</code></li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ConfigMap</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">bird-cm</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">purelb</span><br><span class="hljs-comment"># 中间略过一堆配置</span><br>    <span class="hljs-string">protocol</span> <span class="hljs-string">bgp</span> <span class="hljs-string">uplink1</span> &#123;<br>      <span class="hljs-string">description</span> <span class="hljs-string">&quot;10-31-254-251&quot;</span><span class="hljs-string">;</span><br>      <span class="hljs-string">local</span> <span class="hljs-string">k8sipaddr</span> <span class="hljs-string">as</span> <span class="hljs-number">64515</span><span class="hljs-string">;</span><br>      <span class="hljs-string">neighbor</span> <span class="hljs-number">10.31</span><span class="hljs-number">.254</span><span class="hljs-number">.251</span> <span class="hljs-string">external;</span><br><br>      <span class="hljs-string">ipv4</span> &#123;			<span class="hljs-comment"># IPv4 unicast (1/1)</span><br>        <span class="hljs-comment"># RTS_DEVICE matches routes added to kube-lb0 by protocol device</span><br>        <span class="hljs-string">export</span> <span class="hljs-string">where</span> <span class="hljs-string">source</span> <span class="hljs-string">~</span> [ <span class="hljs-string">RTS_STATIC</span>, <span class="hljs-string">RTS_BGP</span>, <span class="hljs-string">RTS_DEVICE</span> ]<span class="hljs-string">;</span><br>        <span class="hljs-string">import</span> <span class="hljs-string">filter</span> <span class="hljs-string">bgp_reject;</span> <span class="hljs-comment"># we are only advertizing </span><br>      &#125;<span class="hljs-string">;</span><br><br>      <span class="hljs-string">ipv6</span> &#123;			<span class="hljs-comment"># IPv6 unicast </span><br>        <span class="hljs-comment"># RTS_DEVICE matches routes added to kube-lb0 by protocol device</span><br>        <span class="hljs-string">export</span> <span class="hljs-string">where</span>  <span class="hljs-string">source</span> <span class="hljs-string">~</span> [ <span class="hljs-string">RTS_STATIC</span>, <span class="hljs-string">RTS_BGP</span>, <span class="hljs-string">RTS_DEVICE</span> ]<span class="hljs-string">;</span><br>        <span class="hljs-string">import</span> <span class="hljs-string">filter</span> <span class="hljs-string">bgp_reject;</span><br>      &#125;<span class="hljs-string">;</span><br>    &#125;<br></code></pre></div></td></tr></table></figure>

<p>接下来是bird的daemonset配置文件，这里不一定要根据我的步骤修改，大家可以按照实际需求来处理</p>
<ul>
<li><code>namespace</code>：官方默认新建了一个<code>router</code>的<code>namespace</code>来管理，这里我们为了方便统一到<code>purelb</code></li>
<li><code>imagePullPolicy</code>：官方默认是<code>Always</code>，这里我们修改为<code>IfNotPresent</code></li>
</ul>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">bird</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">purelb</span><br><span class="hljs-comment"># 中间略过一堆配置</span><br>        <span class="hljs-attr">image:</span> <span class="hljs-string">registry.gitlab.com/purelb/bird_router:latest</span><br>        <span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">IfNotPresent</span><br></code></pre></div></td></tr></table></figure>

<h2 id="3-2-部署bird"><a href="#3-2-部署bird" class="headerlink" title="3.2 部署bird"></a>3.2 部署bird</h2><p>部署的话非常简单，直接部署上面的两个配置文件即可，注意上面我们把namespace修改为了purelb，因此这里创建namespace这一步可以省略</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash"><span class="hljs-comment"># Create the router namespace</span><br>$ kubectl create namespace router<br><br><span class="hljs-comment"># Apply the edited configmap</span><br>$ kubectl apply -f bird-cm.yml<br><br><span class="hljs-comment"># Deploy the Bird Router</span><br>$ kubectl apply -f bird.yml<br></code></pre></div></td></tr></table></figure>

<p>接着我们检查一下部署的状态</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl get ds -n purelb<br>NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE<br>bird          2         2         2       0            2           &lt;none&gt;                   27m<br>lbnodeagent   3         3         3       3            3           kubernetes.io/os=linux   42h<br><br>$ kubectl get cm -n purelb<br>NAME               DATA   AGE<br>bird-cm            1      28m<br>kube-root-ca.crt   1      42h<br><br>$ kubectl get pods -n purelb<br>NAME                         READY   STATUS    RESTARTS   AGE<br>allocator-5bf9ddbf9b-p976d   1/1     Running   0          42h<br>bird-4qtrm                   1/1     Running   0          16s<br>bird-z9cq2                   1/1     Running   0          49s<br>lbnodeagent-df2hn            1/1     Running   0          42h<br>lbnodeagent-jxn9h            1/1     Running   0          42h<br>lbnodeagent-xn8dz            1/1     Running   0          42h<br></code></pre></div></td></tr></table></figure>

<blockquote>
<p>默认情况下bird不会调度到master节点，这样可以保证master节点不参与到ECMP的负载均衡中，减少master节点上面的网络流量从而提高master的稳定性</p>
<p>如果想让master也参与到ECMP中，可以在bird.yaml的daemonset配置中新增如下配置</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">tolerations:</span><br><span class="hljs-bullet">-</span> <span class="hljs-attr">effect:</span> <span class="hljs-string">NoSchedule</span><br>  <span class="hljs-attr">key:</span> <span class="hljs-string">node-role.kubernetes.io/master</span><br></code></pre></div></td></tr></table></figure>


</blockquote>
<h2 id="3-3-配置路由器"><a href="#3-3-配置路由器" class="headerlink" title="3.3 配置路由器"></a>3.3 配置路由器</h2><p>路由器我们还是使用frr来进行配置</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">root@tiny-openwrt-plus:~<span class="hljs-comment"># cat /etc/frr/frr.conf</span><br>frr version 8.2.2<br>frr defaults traditional<br>hostname tiny-openwrt-plus<br><span class="hljs-built_in">log</span> file /home/frr/frr.log<br><span class="hljs-built_in">log</span> syslog<br>password zebra<br>!<br>router bgp 64512<br> bgp router-id 10.31.254.251<br> no bgp ebgp-requires-policy<br> !<br> neighbor 10.31.188.11 remote-as 64515<br> neighbor 10.31.188.11 description 10-31-188-11<br> neighbor 10.31.188.12 remote-as 64515<br> neighbor 10.31.188.12 description 10-31-188-12<br> !<br> !<br> address-family ipv4 unicast<br> !maximum-paths 3<br> exit-address-family<br><span class="hljs-built_in">exit</span><br>!<br>access-list vty seq 5 permit 127.0.0.0/8<br>access-list vty seq 10 deny any<br>!<br>line vty<br> access-class vty<br><span class="hljs-built_in">exit</span><br>!<br></code></pre></div></td></tr></table></figure>

<p>配置完成之后我们重启服务，然后查看路由器这端的BGP状态，这时候看到和两个worker节点之间的BGP状态建立正常就说明配置没有问题</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">tiny-openwrt-plus<span class="hljs-comment"># show ip bgp summary</span><br><br>IPv4 Unicast Summary (VRF default):<br>BGP router identifier 10.31.254.251, <span class="hljs-built_in">local</span> AS number 64512 vrf-id 0<br><br>Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt Desc<br>10.31.188.11    4      64515         3         4        0    0    0 00:00:13            0        3 10-31-188-11<br>10.31.188.12    4      64515         3         4        0    0    0 00:00:13            0        3 10-31-188-12<br><br></code></pre></div></td></tr></table></figure>

<h2 id="3-4-创建ServiceGroup"><a href="#3-4-创建ServiceGroup" class="headerlink" title="3.4 创建ServiceGroup"></a>3.4 创建ServiceGroup</h2><p>我们还需要给BGP模式创建一个ServiceGroup，用于管理BGP网段的IP，建议IP段使用和k8s的宿主机节点不同网段的IP</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">purelb.io/v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">ServiceGroup</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">bgp-ippool</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">purelb</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">local:</span><br>    <span class="hljs-attr">v4pool:</span><br>      <span class="hljs-attr">subnet:</span> <span class="hljs-string">&#x27;10.189.0.0/16&#x27;</span><br>      <span class="hljs-attr">pool:</span> <span class="hljs-string">&#x27;10.189.0.0-10.189.255.254&#x27;</span><br>      <span class="hljs-attr">aggregation:</span> <span class="hljs-string">/32</span><br></code></pre></div></td></tr></table></figure>

<p>完成之后我们直接部署并检查</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl apply -f purelb-sg-bgp.yaml<br>servicegroup.purelb.io/bgp-ippool created<br><br>$ kubectl get sg -n purelb<br>NAME            AGE<br>bgp-ippool      7s<br>layer2-ippool   41h<br></code></pre></div></td></tr></table></figure>

<h2 id="3-5-部署测试服务"><a href="#3-5-部署测试服务" class="headerlink" title="3.5 部署测试服务"></a>3.5 部署测试服务</h2><p>这里我们还是直接使用上面已经创建的<code>nginx-lb</code>这个<code>deployments</code>，然后直接新建两个service进行测试</p>
<figure class="highlight yaml"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs yaml"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">purelb.io/service-group:</span> <span class="hljs-string">bgp-ippool</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-lb5-service</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">allocateLoadBalancerNodePorts:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">externalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">internalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-lb</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for service access port</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for pod access port</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">LoadBalancer</span><br><br><br><span class="hljs-meta">---</span><br><span class="hljs-meta"></span><br><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span><br><span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span><br><span class="hljs-attr">metadata:</span><br>  <span class="hljs-attr">annotations:</span><br>    <span class="hljs-attr">purelb.io/service-group:</span> <span class="hljs-string">bgp-ippool</span><br>  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-lb6-service</span><br>  <span class="hljs-attr">namespace:</span> <span class="hljs-string">nginx-quic</span><br><span class="hljs-attr">spec:</span><br>  <span class="hljs-attr">allocateLoadBalancerNodePorts:</span> <span class="hljs-literal">false</span><br>  <span class="hljs-attr">externalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">internalTrafficPolicy:</span> <span class="hljs-string">Cluster</span><br>  <span class="hljs-attr">selector:</span><br>    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx-lb</span><br>  <span class="hljs-attr">ports:</span><br>  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span><br>    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for service access port</span><br>    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span> <span class="hljs-comment"># match for pod access port</span><br>  <span class="hljs-attr">type:</span> <span class="hljs-string">LoadBalancer</span><br>  <span class="hljs-attr">loadBalancerIP:</span> <span class="hljs-number">10.189</span><span class="hljs-number">.100</span><span class="hljs-number">.100</span><br></code></pre></div></td></tr></table></figure>

<p>此时我们检查部署的状态</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">$ kubectl get svc -n nginx-quic<br>NAME                 TYPE           CLUSTER-IP      EXTERNAL-IP      PORT(S)          AGE<br>nginx-lb-service     LoadBalancer   10.188.54.81    10.31.188.64     80/TCP           40h<br>nginx-lb2-service    LoadBalancer   10.188.34.171   10.31.188.65     80/TCP           40h<br>nginx-lb3-service    LoadBalancer   10.188.6.24     10.31.188.66     80/TCP           40h<br>nginx-lb4-service    LoadBalancer   10.188.50.164   10.31.188.100    80/TCP           40h<br>nginx-lb5-service    LoadBalancer   10.188.7.75     10.189.0.0       80/TCP           11s<br>nginx-lb6-service    LoadBalancer   10.188.27.208   10.189.100.100   80/TCP           11s<br></code></pre></div></td></tr></table></figure>

<p>再使用curl进行测试</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.100.100</span><br>10.0.1.47:57768<br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.100.100</span><br>10.0.1.47:57770<br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.100.100</span><br>10.31.188.11:47439<br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.100.100</span><br>10.31.188.11:33964<br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.100.100</span><br>10.0.1.47:57776<br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.100.100</span><br>10.0.1.47:57778<br><br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.0.0</span><br>10.31.188.12:53078<br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.0.0</span><br>10.0.2.151:59660<br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.0.0</span><br>10.0.2.151:59662<br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.0.0</span><br>10.31.188.12:21972<br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.0.0</span><br>10.31.188.12:28855<br>[root@tiny-centos7-100-2 ~]<span class="hljs-comment"># curl 10.189.0.0</span><br>10.0.2.151:59668<br></code></pre></div></td></tr></table></figure>

<p>然后我们再查看kube-lb0网卡上面的IP信息，可以看到每台节点上面都有两个BGP模式的LoadBalancer的IP</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">[tinychen /root/ansible]<span class="hljs-comment"># ansible cilium -m command -a &quot;ip addr show kube-lb0&quot;</span><br>10.31.188.11 | CHANGED | rc=0 &gt;&gt;<br>19: kube-lb0: &lt;BROADCAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000<br>    link/ether d6:65:b8:31:18:ce brd ff:ff:ff:ff:ff:ff<br>    inet 10.189.0.0/32 scope global kube-lb0<br>       valid_lft forever preferred_lft forever<br>    inet 10.189.100.100/32 scope global kube-lb0<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::d465:b8ff:fe31:18ce/64 scope link<br>       valid_lft forever preferred_lft forever<br>10.31.188.12 | CHANGED | rc=0 &gt;&gt;<br>21: kube-lb0: &lt;BROADCAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000<br>    link/ether aa:10:d5:<span class="hljs-built_in">cd</span>:2b:98 brd ff:ff:ff:ff:ff:ff<br>    inet 10.189.0.0/32 scope global kube-lb0<br>       valid_lft forever preferred_lft forever<br>    inet 10.189.100.100/32 scope global kube-lb0<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::a810:d5ff:fecd:2b98/64 scope link<br>       valid_lft forever preferred_lft forever<br>10.31.188.1 | CHANGED | rc=0 &gt;&gt;<br>15: kube-lb0: &lt;BROADCAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default qlen 1000<br>    link/ether 12:27:b1:48:4e:3a brd ff:ff:ff:ff:ff:ff<br>    inet 10.189.0.0/32 scope global kube-lb0<br>       valid_lft forever preferred_lft forever<br>    inet 10.189.100.100/32 scope global kube-lb0<br>       valid_lft forever preferred_lft forever<br>    inet6 fe80::1027:b1ff:fe48:4e3a/64 scope link<br>       valid_lft forever preferred_lft forever<br></code></pre></div></td></tr></table></figure>

<p>最后我们查看路由器上面的路由表，可以确定ECMP开启成功</p>
<figure class="highlight bash"><table><tr><td class="gutter hljs"><div class="hljs code-wrapper"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></div></td><td class="code"><div class="hljs code-wrapper"><pre><code class="hljs bash">tiny-openwrt-plus<span class="hljs-comment"># show ip route</span><br>Codes: K - kernel route, C - connected, S - static, R - RIP,<br>       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,<br>       T - Table, v - VNC, V - VNC-Direct, A - Babel, F - PBR,<br>       f - OpenFabric,<br>       &gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup<br>       t - trapped, o - offload failure<br><br>K&gt;* 0.0.0.0/0 [0/0] via 10.31.254.254, eth0, 00:08:51<br>C&gt;* 10.31.0.0/16 is directly connected, eth0, 00:08:51<br>B&gt;* 10.189.0.0/32 [20/0] via 10.31.188.11, eth0, weight 1, 00:00:19<br>  *                      via 10.31.188.12, eth0, weight 1, 00:00:19<br>B&gt;* 10.189.100.100/32 [20/0] via 10.31.188.11, eth0, weight 1, 00:00:19<br>  *                          via 10.31.188.12, eth0, weight 1, 00:00:19<br></code></pre></div></td></tr></table></figure>

<h1 id="4、总结"><a href="#4、总结" class="headerlink" title="4、总结"></a>4、总结</h1><p>PureLB和前面我们提到过的MetalLB以及OpenELB有着非常大的不同，尽管三者的主要工作模式都是分为Layer2模式和BGP模式。还是老规矩，我们先来看两种工作模式的优缺点，再来总结PureLB。</p>
<h2 id="4-1-Layer2-mode优缺点"><a href="#4-1-Layer2-mode优缺点" class="headerlink" title="4.1 Layer2 mode优缺点"></a>4.1 Layer2 mode优缺点</h2><p><strong>优点：</strong></p>
<ul>
<li>通用性强，对比BGP模式不需要BGP路由器支持，几乎可以适用于任何网络环境；当然云厂商的网络环境例外</li>
<li>VIP会被分散到多个节点上面，解决了MetalLB和OpenELB的Layer2模式下的流量单点瓶颈问题</li>
<li>使用了Linux网络栈，可以通过iproute之类的命令直接查看到vip所在的节点</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>当VIP所在节点宕机之后，需要较长时间进行故障转移（官方没说多久），PureLB和MetalLB一样都使用了<a target="_blank" rel="noopener" href="https://github.com/hashicorp/memberlist">memberlist</a>来进行选主（并表示此举更优），当VIP所在节点宕机之后重新选主的时间要比传统的keepalived使用的<code>vrrp</code>协议（一般为1s）要更长</li>
</ul>
<p><strong>改进方案：</strong></p>
<ul>
<li>有条件的可以考虑使用BGP模式</li>
<li>可以针对一个负载workload创建多个service，并对外暴露多个VIP，由于PureLB会把VIP分散到多个节点上，这样可以一定程度上实现高可用</li>
<li>既不能用BGP模式也不能接受Layer2模式的，基本和目前主流的三个开源负载均衡器无缘了（三者都是Layer2模式和BGP模式且原理类似，优缺点相同）</li>
</ul>
<h2 id="4-2-ECMP-mode优缺点"><a href="#4-2-ECMP-mode优缺点" class="headerlink" title="4.2 ECMP mode优缺点"></a>4.2 ECMP mode优缺点</h2><p>ECMP模式的优缺点几乎和Layer2模式相反</p>
<p><strong>优点：</strong></p>
<ul>
<li>无单点故障，在开启ECMP的前提下，k8s集群内所有的节点都有请求流量，都会参与负载均衡并转发请求</li>
<li>支持了Linux网络栈，因此可以使用bird、quagga、frr等各种路由软件实现标准的路由协议</li>
</ul>
<p><strong>缺点：</strong></p>
<ul>
<li>条件苛刻，需要有特殊路由器支持，配置起来也更复杂；</li>
<li>ECMP的故障转移（failover）并不是特别地优雅，这个问题的严重程度取决于使用的ECMP算法；当集群的节点出现变动导致BGP连接出现变动，所有的连接都会进行重新哈希（使用三元组或五元组哈希），这对一些服务来说可能会有影响；</li>
</ul>
<blockquote>
<p>路由器中使用的哈希值通常 <em>不稳定</em>，因此每当后端集的大小发生变化时（例如，当一个节点的 BGP 会话关闭时），现有的连接将被有效地随机重新哈希，这意味着大多数现有的连接最终会突然被转发到不同的后端，而这个后端可能和此前的后端毫不相干且不清楚上下文状态信息。</p>
</blockquote>
<p><strong>改进方案：</strong></p>
<p>PureLB官方只简单提及了使用路由协议的<a target="_blank" rel="noopener" href="https://purelb.gitlab.io/docs/how_it_works/routers/">一些问题</a>：</p>
<blockquote>
<p>Depending on the router and its configuration, load balancing techniques will vary however they are all generally based upon a 4 tuple hash of sourceIP, sourcePort, destinationIP, destinationPort. The router will also have a limit to the number of ECMP paths that can be used, in modern TOR switches, this can be set to a size larger than a /24 subnet, however in old routers, the count can be less than 10. This needs to be considered in the infrastructure design and PureLB combined with routing software can help create a design that avoids this limitation. Another important consideration can be how the router load balancer cache is populated and updated when paths are removed, again modern devices provide better behavior.</p>
</blockquote>
<p>不过由于都是使用ECMP，我们可以参考MetalLB官方给出的资料，下面是MetalLB给出的一些<a target="_blank" rel="noopener" href="https://metallb.universe.tf/concepts/bgp/#limitations">改进方案</a>，列出来给大家参考一下</p>
<ul>
<li>使用更稳定的ECMP算法来减少后端变动时对现有连接的影响，如“resilient ECMP” or “resilient LAG”</li>
<li>将服务部署到特定的节点上减少可能带来的影响</li>
<li>在流量低峰期进行变更</li>
<li>将服务分开部署到两个不同的LoadBalanceIP的服务中，然后利用DNS进行流量切换</li>
<li>在客户端加入透明的用户无感的重试逻辑</li>
<li>在LoadBalance后面加入一层ingress来实现更优雅的failover（但是并不是所有的服务都可以使用ingress）</li>
<li>接受现实……（Accept that there will be occasional bursts of reset connections. For low-availability internal services, this may be acceptable as-is.）</li>
</ul>
<h2 id="4-3-PureLB优缺点"><a href="#4-3-PureLB优缺点" class="headerlink" title="4.3 PureLB优缺点"></a>4.3 PureLB优缺点</h2><p>这里尽量客观的总结概况一些客观事实，是否为优缺点可能会因人而异：</p>
<ul>
<li>PureLB使用了CRD来实现更优秀的IPAM，也是三者中唯一一个支持外置IPAM的</li>
<li>PureLB对Linux网络栈有更好的支持（可以使用iproute等工具查看LoadBalancerVIP）</li>
<li>PureLB可以使用任意路由协议实现ECMP（BGP、OSPF等）</li>
<li>PureLB和使用BGP模式的CNI集成更加方便</li>
<li>PureLB的社区热度不如MetalLB和OpenELB，也没有加入CNCF，只表示CNCF提供了一个slack通道给用户进行交流（The CNCF have generously provided the PureLB community a Slack Channel in the Kubernetes workspace.）</li>
<li>PureLB的文档相对齐全，但是还是有些小纰漏</li>
<li>PureLB的Layer2模式不存在单点流量瓶颈</li>
</ul>
<p>总的来说PureLB是一款非常不错的云原生负载均衡器，在软件本身的设计模式上面应该是参考了MetalLB等前辈的思路，同时又青出于蓝而胜于蓝。唯一美中不足的是社区热度不高，让人有些担心这个项目以后的发展情况。如果在三者中选一个使用layer2模式的话，个人推荐首选PureLB；如果是使用BGP模式，则建议结合自己的CNI组件和IPAM等情况综合考虑。</p>

            </div>
            <hr>
            <div>
              <div class="post-metas mb-3">
                
                  <div class="post-meta mr-3">
                    <i class="iconfont icon-category"></i>
                    
                      <a class="hover-with-bg" href="/categories/cloudnative/">cloudnative</a>
                    
                  </div>
                
                
                  <div class="post-meta">
                    <i class="iconfont icon-tags"></i>
                    
                      <a class="hover-with-bg" href="/tags/k8s/">k8s</a>
                    
                      <a class="hover-with-bg" href="/tags/loadbalance/">loadbalance</a>
                    
                      <a class="hover-with-bg" href="/tags/metallb/">metallb</a>
                    
                      <a class="hover-with-bg" href="/tags/openelb/">openelb</a>
                    
                      <a class="hover-with-bg" href="/tags/purelb/">purelb</a>
                    
                      <a class="hover-with-bg" href="/tags/bgp/">bgp</a>
                    
                      <a class="hover-with-bg" href="/tags/quagga/">quagga</a>
                    
                      <a class="hover-with-bg" href="/tags/frr/">frr</a>
                    
                  </div>
                
              </div>
              
              
                <div class="post-prevnext">
                  <article class="post-prev col-6">
                    
                    
                      <a href="/20220627-k8s-09-service-discovery-and-traffic-exposure/">
                        <i class="iconfont icon-arrowleft"></i>
                        <span class="hidden-mobile">k8s系列09-服务发现与流量暴露</span>
                        <span class="visible-mobile">Previous</span>
                      </a>
                    
                  </article>
                  <article class="post-next col-6">
                    
                    
                      <a href="/20220523-k8s-07-loadbalancer-openelb/">
                        <span class="hidden-mobile">k8s系列07-负载均衡器之OpenELB</span>
                        <span class="visible-mobile">Next</span>
                        <i class="iconfont icon-arrowright"></i>
                      </a>
                    
                  </article>
                </div>
              
            </div>

            
          </article>
        </div>
      </div>
    </div>
    
      <div class="d-none d-lg-block col-lg-2 toc-container" id="toc-ctn">
        <div id="toc">
  <p class="toc-header"><i class="iconfont icon-list"></i>&nbsp;TOC</p>
  <div class="toc-body" id="toc-body"></div>
</div>

      </div>
    
  </div>
</div>

<!-- Custom -->


    

    
      <a id="scroll-top-button" aria-label="TOP" href="#" role="button">
        <i class="iconfont icon-arrowup" aria-hidden="true"></i>
      </a>
    

    
      <div class="modal fade" id="modalSearch" tabindex="-1" role="dialog" aria-labelledby="ModalLabel"
     aria-hidden="true">
  <div class="modal-dialog modal-dialog-scrollable modal-lg" role="document">
    <div class="modal-content">
      <div class="modal-header text-center">
        <h4 class="modal-title w-100 font-weight-bold">Search</h4>
        <button type="button" id="local-search-close" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body mx-3">
        <div class="md-form mb-5">
          <input type="text" id="local-search-input" class="form-control validate">
          <label data-error="x" data-success="v"
                 for="local-search-input">keyword</label>
        </div>
        <div class="list-group" id="local-search-result"></div>
      </div>
    </div>
  </div>
</div>
    

    
  </main>

  <footer class="text-center mt-5 py-3">
  <div class="footer-content">
     <i class="iconfont icon-copyright"></i> <a href="https://tinychen.com/" target="_blank" rel="nofollow noopener"><span>2017~2023 By TinyChen </span></a> <i class="iconfont icon-love"></i> <a href="https://github.com/fluid-dev/hexo-theme-fluid" target="_blank" rel="nofollow noopener"><span>Hexo-Fluid</span></a> 
  </div>
  
  <div class="statistics">
    
    

    
      
        <!-- 不蒜子统计PV -->
        <span id="busuanzi_container_site_pv" style="display: none">
            总访问量 
            <span id="busuanzi_value_site_pv"></span>
             次
          </span>
      
      
        <!-- 不蒜子统计UV -->
        <span id="busuanzi_container_site_uv" style="display: none">
            总访客数 
            <span id="busuanzi_value_site_uv"></span>
             人
          </span>
      
    
  </div>


  
  <!-- 备案信息 -->
  <div class="beian">
    <span>
      <a href="http://beian.miit.gov.cn/" target="_blank" rel="nofollow noopener">
        粤ICP备18140640号
      </a>
    </span>
    
  </div>


  
</footer>


  <!-- SCRIPTS -->
  
  <script  src="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.js" ></script>
  <link  rel="stylesheet" href="https://cdn.jsdelivr.net/npm/nprogress@0/nprogress.min.css" />

  <script>
    NProgress.configure({"showSpinner":false,"trickleSpeed":100})
    NProgress.start()
    window.addEventListener('load', function() {
      NProgress.done();
    })
  </script>


<script  src="https://cdn.jsdelivr.net/npm/jquery@3/dist/jquery.min.js" ></script>
<script  src="https://cdn.jsdelivr.net/npm/bootstrap@4/dist/js/bootstrap.min.js" ></script>
<script  src="/js/events.js" ></script>
<script  src="/js/plugins.js" ></script>

<!-- Plugins -->


  <script  src="/js/local-search.js" ></script>



  
    <script  src="/js/img-lazyload.js" ></script>
  



  



  
    <script  src="https://cdn.jsdelivr.net/npm/tocbot@4/dist/tocbot.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@3/dist/jquery.fancybox.min.js" ></script>
  
  
    <script  src="https://cdn.jsdelivr.net/npm/anchor-js@4/anchor.min.js" ></script>
  
  
    <script defer src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js" ></script>
  



  <script defer src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" ></script>




  <script  src="https://cdn.jsdelivr.net/npm/typed.js@2/lib/typed.min.js" ></script>
  <script>
    (function (window, document) {
      var typing = Fluid.plugins.typing;
      var title = document.getElementById('subtitle').title;
      
        typing(title);
      
    })(window, document);
  </script>












  
    <!-- Baidu Analytics -->
    <script defer>
      var _hmt = _hmt || [];
      (function () {
        var hm = document.createElement("script");
        hm.src = "https://hm.baidu.com/hm.js?7a96963a1145ac7fde1442d739a11ffd";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
    </script>
  

  
    <!-- Google Analytics -->
    <script defer>
      window.ga = window.ga || function () { (ga.q = ga.q || []).push(arguments) };
      ga.l = +new Date;
      ga('create', 'UA-166769908-1', 'auto');
      ga('send', 'pageview');
    </script>
    <script async src='https://www.google-analytics.com/analytics.js'></script>
  

  

  

  

  





<!-- 主题的启动项 保持在最底部 -->
<script  src="/js/boot.js" ></script>


</body>
</html>
